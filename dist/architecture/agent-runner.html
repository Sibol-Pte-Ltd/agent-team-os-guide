<!DOCTYPE html><!--VA_lAVEn1LekIejhMlH5y--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/56998b7e09014914.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-d282787ed3063711.js"/><script src="/_next/static/chunks/4bd1b696-f785427dddbba9fb.js" async=""></script><script src="/_next/static/chunks/1255-29325acccda77703.js" async=""></script><script src="/_next/static/chunks/main-app-035730848130ded3.js" async=""></script><script src="/_next/static/chunks/app/(authenticated)/layout-8a56e70ab4050a46.js" async=""></script><title>Agent Runner — OpenClaw Architecture</title><meta name="description" content="Your personal AI agent team, working together to keep your life organized."/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body><div hidden=""><!--$--><!--/$--></div><input type="checkbox" id="menu-toggle" class="menu-toggle"/><label for="menu-toggle" class="hamburger" aria-label="Toggle navigation">☰</label><label for="menu-toggle" class="overlay"></label><div class="site-wrapper"><aside class="sidebar"><div class="sidebar-header"><h1>Agent Team OS</h1><div class="subtitle">User Guide</div></div><nav><div class="nav-section"><a class="nav-link" href="/">Home</a></div><div class="nav-section"><div class="nav-section-title">Getting Started</div><a class="nav-link" href="/getting-started">Quick Start</a><a class="nav-link" href="/getting-started/first-day">Your First Day</a><a class="nav-link" href="/getting-started/setup-checklist">Setup Checklist</a></div><div class="nav-section"><div class="nav-section-title">Notes &amp; Sync</div><a class="nav-link" href="/life-os">What is Notes</a><a class="nav-link" href="/life-os/daily-workflow">Daily Workflow</a><a class="nav-link" href="/life-os/templates">Templates</a><a class="nav-link" href="/life-os/logseq-tips">LogSeq Tips</a></div><div class="nav-section"><div class="nav-section-title">Agents</div><a class="nav-link" href="/agents">Meet Your Agents</a><a class="nav-link" href="/agents/ember">Ember</a><a class="nav-link" href="/agents/scout">Scout</a><a class="nav-link" href="/agents/forge">Forge</a><a class="nav-link" href="/agents/architect">Architect</a><a class="nav-link" href="/agents/trust-levels">Trust Levels</a></div><div class="nav-section"><div class="nav-section-title">Cadence</div><a class="nav-link" href="/cadence">Overview</a><a class="nav-link" href="/cadence/morning-report">Morning Report</a><a class="nav-link" href="/cadence/evening-checkin">Evening Check-in</a><a class="nav-link" href="/cadence/weekly-review">Weekly Review</a><a class="nav-link" href="/cadence/monthly-review">Monthly Review</a></div><div class="nav-section"><div class="nav-section-title">OpenClaw Architecture</div><a class="nav-link" href="/architecture">Overview</a><a class="nav-link" href="/architecture/channel-adapters">Channel Adapters</a><a class="nav-link" href="/architecture/gateway">Gateway</a><a class="nav-link" href="/architecture/lane-queue">Lane Queue</a><a class="nav-link active" href="/architecture/agent-runner">Agent Runner</a><a class="nav-link" href="/architecture/execution-layer">Execution Layer</a></div><div class="nav-section"><div class="nav-section-title">Automation</div><a class="nav-link" href="/automation">Overview</a><a class="nav-link" href="/automation/cron-jobs">Cron Jobs</a><a class="nav-link" href="/automation/notifications">Notifications</a></div><div class="nav-section"><div class="nav-section-title">Troubleshooting</div><a class="nav-link" href="/troubleshooting">Common Issues</a><a class="nav-link" href="/troubleshooting/syncthing">Syncthing</a><a class="nav-link" href="/troubleshooting/faq">FAQ</a></div><div class="nav-section"><div class="nav-section-title">Reference</div><a class="nav-link" href="/reference/commands">Commands</a><a class="nav-link" href="/reference/file-locations">File Locations</a><a class="nav-link" href="/reference/glossary">Glossary</a><a class="nav-link" href="/reference/changelog">Changelog</a></div><div class="nav-section"><div class="nav-section-title">Implementation</div><a class="nav-link" href="/implementation">Status</a><a class="nav-link" href="/implementation/gap-analysis">Gap Analysis</a><a class="nav-link" href="/implementation/roadmap">Roadmap</a><a class="nav-link" href="/implementation/activity-log">Activity Log</a><a class="nav-link" href="/implementation/site-improvements">Site Improvements</a><a class="nav-link" href="/implementation/clawvault-analysis">ClawVault Analysis</a></div></nav><div class="sidebar-footer"><button class="nav-link logout-link">⎋ Logout</button></div></aside><main class="main-content"><div class="content-wrapper"><div><div class="breadcrumbs">
          <a href="/">Home</a>
          <span class="separator">/</span>
          <a href="/architecture">Architecture</a>
          <span class="separator">/</span>
          Agent Runner
        </div>

        <h1>Agent Runner (Agent Loop)</h1>

        <p>The Agent Runner is where the intelligence lives. It is the component that orchestrates the full reasoning cycle: assembling context, calling the LLM, interpreting its response, executing any requested tool calls, feeding results back, and repeating until the agent produces a final answer. This iterative loop — reason, act, observe, repeat — is what transforms a raw language model into a capable assistant that can research, code, browse the web, and manage your life.</p>

        <h2>The Agent Loop</h2>

        <p>When the Lane Queue dequeues a message for processing, the Agent Runner takes over. Here is what happens in each cycle of the loop:</p>

        <ol class="steps">
          <li>
            <strong>Build context.</strong> The Runner assembles everything the LLM needs to generate a response:
            <ul>
              <li><em>System prompt</em> — the agent's identity, instructions, available tools, workspace context, runtime information, and behavioral rules</li>
              <li><em>Conversation history</em> — previous messages in this session, loaded from the transcript file</li>
              <li><em>Bootstrap files</em> — workspace files like AGENTS.md, TOOLS.md, SOUL.md, and memory files that provide the agent's persistent context</li>
              <li><em>User message</em> — the new inbound message that triggered this run</li>
            </ul>
          </li>
          <li>
            <strong>Call the LLM.</strong> The assembled messages are sent to the configured language model (e.g., Claude, GPT-4, Gemini) via its API. The model receives the full context and produces a response — which may be plain text, a tool call, or a combination.
          </li>
          <li>
            <strong>Handle the response.</strong> If the model returns plain text, the loop ends and the text becomes the agent's reply. If the model returns one or more tool calls, the Runner proceeds to step 4.
          </li>
          <li>
            <strong>Execute tool calls.</strong> Each tool call is dispatched to the Execution Layer. The tool's function runs, produces a result (success or error), and the result is captured as a structured tool response.
          </li>
          <li>
            <strong>Feed results back.</strong> The tool results are appended to the conversation as tool response messages, and the loop returns to step 2 — the LLM sees what the tools returned and decides what to do next.
          </li>
          <li>
            <strong>Repeat until complete.</strong> The loop continues until the model produces a final text response without requesting any more tool calls. Complex tasks might loop dozens of times (read a file, edit it, run a test, read the output, fix an error, run again…).
          </li>
        </ol>

        <h2>System Prompt Assembly</h2>

        <p>The system prompt is one of the most critical pieces of the Agent Runner. It is dynamically assembled for each run based on the agent's configuration and the current context. It includes:</p>

        <ul>
          <li><strong>Identity</strong> — the agent's name, persona description, and channel-specific identity if configured</li>
          <li><strong>Tool catalog</strong> — descriptions and schemas for every tool the agent can use in this session</li>
          <li><strong>Workspace context</strong> — bootstrap files (AGENTS.md, SOUL.md, USER.md, TOOLS.md, memory files) injected directly into the prompt</li>
          <li><strong>Runtime info</strong> — current date/time, timezone, OS details, model name, available capabilities, channel type</li>
          <li><strong>Skills</strong> — available skill descriptions with instructions for the agent to read the relevant SKILL.md before using a skill</li>
          <li><strong>Memory</strong> — instructions for using memory search and retrieval tools</li>
          <li><strong>Behavioral rules</strong> — reply formatting, silent reply tokens, heartbeat behavior, reply tags, messaging guidelines</li>
          <li><strong>Subagent context</strong> — if this is a subagent session, additional context about the parent task and reporting instructions</li>
        </ul>

        <p>The prompt mode varies: main agents get the full prompt with all sections; subagents get a minimal prompt focused on their specific task; and special modes like "none" provide just a basic identity line.</p>

        <h2>Model Selection &amp; Failover</h2>

        <p>The Agent Runner supports sophisticated model management:</p>

        <ul>
          <li><strong>Per-agent model config</strong> — different agents can use different models (Ember on Sonnet, Architect on Opus)</li>
          <li><strong>Inline model switching</strong> — users can switch models mid-conversation using directives like <code>/model opus</code></li>
          <li><strong>Auth profile rotation</strong> — multiple API keys for the same provider, with automatic rotation on rate limits</li>
          <li><strong>Automatic failover</strong> — if a model call fails (rate limit, timeout, context overflow), the Runner retries with fallback models or providers</li>
          <li><strong>Thinking/reasoning levels</strong> — configurable reasoning depth (off, low, medium, high) that maps to provider-specific extended thinking features</li>
          <li><strong>Context window management</strong> — automatic compaction when conversation history approaches the model's context limit</li>
        </ul>

        <h2>Streaming &amp; Block Replies</h2>

        <p>The Agent Runner streams responses in real time. As the LLM generates tokens, they are:</p>

        <ul>
          <li><strong>Chunked into paragraphs</strong> — the block chunker accumulates tokens and flushes complete paragraphs to the Channel Adapter, so users see responses appearing in natural-feeling blocks rather than character by character</li>
          <li><strong>Filtered for thinking tags</strong> — reasoning/thinking content (from models like Claude with extended thinking) is handled according to the configured reasoning mode: suppressed, streamed separately, or included in the reply</li>
          <li><strong>Checked for reply directives</strong> — inline directives like model switches, reply-to tags, and status commands are parsed and handled during streaming</li>
          <li><strong>Tracked for tool calls</strong> — tool call delimiters are detected in the stream so tool execution can begin as soon as the model finishes specifying the call</li>
        </ul>

        <h2>Conversation History Management</h2>

        <p>Each session maintains a transcript file on disk. The Agent Runner manages this transcript across runs:</p>

        <ul>
          <li><strong>History limiting</strong> — only the most recent N turns are included in the LLM context, configurable per agent and per channel</li>
          <li><strong>Compaction</strong> — when the transcript grows too large for the context window, the system summarizes older messages and replaces them with a compact summary, preserving key context while freeing token space</li>
          <li><strong>Tool result truncation</strong> — oversized tool outputs in the history are automatically truncated to prevent context overflow on subsequent runs</li>
          <li><strong>Transcript repair</strong> — malformed transcripts (from crashes or interrupted writes) are automatically detected and repaired on load</li>
        </ul>

        <h2>How It Works in Code</h2>

        <p>The Agent Runner is implemented across several key files in <code>src/agents/</code>:</p>

        <ul>
          <li><code>pi-embedded-runner/run.ts</code> — the main entry point: <code>runEmbeddedPiAgent()</code> orchestrates the full run lifecycle including auth resolution, failover, and context assembly</li>
          <li><code>pi-embedded-runner/runs.ts</code> — manages the registry of active runs, message queueing to active sessions, and abort handling</li>
          <li><code>pi-embedded-subscribe.ts</code> — the streaming subscription layer that processes LLM output events (message chunks, tool calls, reasoning tokens) and dispatches them to the appropriate handlers</li>
          <li><code>pi-embedded-subscribe.handlers.ts</code> — event handlers for the six LLM event types: message_start, message_update, message_end, tool_execution_start, tool_execution_update, tool_execution_end</li>
          <li><code>system-prompt.ts</code> — the system prompt builder that assembles all context sections</li>
          <li><code>pi-tools.ts</code> — tool catalog construction: which tools are available, their schemas, and their policy-based filtering</li>
          <li><code>compaction.ts</code> — conversation compaction logic for managing context window limits</li>
        </ul>

        <div class="callout tip">
          <div class="callout-title">The Loop Is the Agent</div>
          <p>An LLM by itself is just a text-completion engine. The Agent Runner is what transforms it into an agent — the ability to observe a situation, decide on an action, execute it, observe the result, and decide again. This loop is the fundamental mechanism that enables complex, multi-step tasks like "research this topic, write a report, and email it to me." Each step requires the model to see what happened in the previous step and decide what to do next.</p>
        </div>

        <div class="action-section">
          <h2>Next</h2>
          <ul>
            <li><a href="/architecture/execution-layer">Execution Layer</a> — how tool calls are actually executed</li>
            <li><a href="/architecture/lane-queue">Lane Queue</a> — what happens before the Runner gets the message</li>
            <li><a href="/architecture">Architecture Overview</a> — see how all five components connect</li>
          </ul>
        </div></div><!--$--><!--/$--></div></main></div><script src="/_next/static/chunks/webpack-d282787ed3063711.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[9766,[],\"\"]\n3:I[8924,[],\"\"]\n4:I[3370,[\"7340\",\"static/chunks/app/(authenticated)/layout-8a56e70ab4050a46.js\"],\"default\"]\n8:I[7150,[],\"\"]\n:HL[\"/_next/static/css/56998b7e09014914.css\",\"style\"]\n5:T27e2,"])</script><script>self.__next_f.push([1,"\u003cdiv class=\"breadcrumbs\"\u003e\n          \u003ca href=\"/\"\u003eHome\u003c/a\u003e\n          \u003cspan class=\"separator\"\u003e/\u003c/span\u003e\n          \u003ca href=\"/architecture\"\u003eArchitecture\u003c/a\u003e\n          \u003cspan class=\"separator\"\u003e/\u003c/span\u003e\n          Agent Runner\n        \u003c/div\u003e\n\n        \u003ch1\u003eAgent Runner (Agent Loop)\u003c/h1\u003e\n\n        \u003cp\u003eThe Agent Runner is where the intelligence lives. It is the component that orchestrates the full reasoning cycle: assembling context, calling the LLM, interpreting its response, executing any requested tool calls, feeding results back, and repeating until the agent produces a final answer. This iterative loop — reason, act, observe, repeat — is what transforms a raw language model into a capable assistant that can research, code, browse the web, and manage your life.\u003c/p\u003e\n\n        \u003ch2\u003eThe Agent Loop\u003c/h2\u003e\n\n        \u003cp\u003eWhen the Lane Queue dequeues a message for processing, the Agent Runner takes over. Here is what happens in each cycle of the loop:\u003c/p\u003e\n\n        \u003col class=\"steps\"\u003e\n          \u003cli\u003e\n            \u003cstrong\u003eBuild context.\u003c/strong\u003e The Runner assembles everything the LLM needs to generate a response:\n            \u003cul\u003e\n              \u003cli\u003e\u003cem\u003eSystem prompt\u003c/em\u003e — the agent's identity, instructions, available tools, workspace context, runtime information, and behavioral rules\u003c/li\u003e\n              \u003cli\u003e\u003cem\u003eConversation history\u003c/em\u003e — previous messages in this session, loaded from the transcript file\u003c/li\u003e\n              \u003cli\u003e\u003cem\u003eBootstrap files\u003c/em\u003e — workspace files like AGENTS.md, TOOLS.md, SOUL.md, and memory files that provide the agent's persistent context\u003c/li\u003e\n              \u003cli\u003e\u003cem\u003eUser message\u003c/em\u003e — the new inbound message that triggered this run\u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\n            \u003cstrong\u003eCall the LLM.\u003c/strong\u003e The assembled messages are sent to the configured language model (e.g., Claude, GPT-4, Gemini) via its API. The model receives the full context and produces a response — which may be plain text, a tool call, or a combination.\n          \u003c/li\u003e\n          \u003cli\u003e\n            \u003cstrong\u003eHandle the response.\u003c/strong\u003e If the model returns plain text, the loop ends and the text becomes the agent's reply. If the model returns one or more tool calls, the Runner proceeds to step 4.\n          \u003c/li\u003e\n          \u003cli\u003e\n            \u003cstrong\u003eExecute tool calls.\u003c/strong\u003e Each tool call is dispatched to the Execution Layer. The tool's function runs, produces a result (success or error), and the result is captured as a structured tool response.\n          \u003c/li\u003e\n          \u003cli\u003e\n            \u003cstrong\u003eFeed results back.\u003c/strong\u003e The tool results are appended to the conversation as tool response messages, and the loop returns to step 2 — the LLM sees what the tools returned and decides what to do next.\n          \u003c/li\u003e\n          \u003cli\u003e\n            \u003cstrong\u003eRepeat until complete.\u003c/strong\u003e The loop continues until the model produces a final text response without requesting any more tool calls. Complex tasks might loop dozens of times (read a file, edit it, run a test, read the output, fix an error, run again…).\n          \u003c/li\u003e\n        \u003c/ol\u003e\n\n        \u003ch2\u003eSystem Prompt Assembly\u003c/h2\u003e\n\n        \u003cp\u003eThe system prompt is one of the most critical pieces of the Agent Runner. It is dynamically assembled for each run based on the agent's configuration and the current context. It includes:\u003c/p\u003e\n\n        \u003cul\u003e\n          \u003cli\u003e\u003cstrong\u003eIdentity\u003c/strong\u003e — the agent's name, persona description, and channel-specific identity if configured\u003c/li\u003e\n          \u003cli\u003e\u003cstrong\u003eTool catalog\u003c/strong\u003e — descriptions and schemas for every tool the agent can use in this session\u003c/li\u003e\n          \u003cli\u003e\u003cstrong\u003eWorkspace context\u003c/strong\u003e — bootstrap files (AGENTS.md, SOUL.md, USER.md, TOOLS.md, memory files) injected directly into the prompt\u003c/li\u003e\n          \u003cli\u003e\u003cstrong\u003eRuntime info\u003c/strong\u003e — current date/time, timezone, OS details, model name, available capabilities, channel type\u003c/li\u003e\n          \u003cli\u003e\u003cstrong\u003eSkills\u003c/strong\u003e — available skill descriptions with instructions for the agent to read the relevant SKILL.md before using a skill\u003c/li\u003e\n          \u003cli\u003e\u003cstrong\u003eMemory\u003c/strong\u003e — instructions for using memory search and retrieval tools\u003c/li\u003e\n          \u003cli\u003e\u003cstrong\u003eBehavioral rules\u003c/strong\u003e — reply formatting, silent reply tokens, heartbeat behavior, reply tags, messaging guidelines\u003c/li\u003e\n          \u003cli\u003e\u003cstrong\u003eSubagent context\u003c/strong\u003e — if this is a subagent session, additional context about the parent task and reporting instructions\u003c/li\u003e\n        \u003c/ul\u003e\n\n        \u003cp\u003eThe prompt mode varies: main agents get the full prompt with all sections; subagents get a minimal prompt focused on their specific task; and special modes like \"none\" provide just a basic identity line.\u003c/p\u003e\n\n        \u003ch2\u003eModel Selection \u0026amp; Failover\u003c/h2\u003e\n\n        \u003cp\u003eThe Agent Runner supports sophisticated model management:\u003c/p\u003e\n\n        \u003cul\u003e\n          \u003cli\u003e\u003cstrong\u003ePer-agent model config\u003c/strong\u003e — different agents can use different models (Ember on Sonnet, Architect on Opus)\u003c/li\u003e\n          \u003cli\u003e\u003cstrong\u003eInline model switching\u003c/strong\u003e — users can switch models mid-conversation using directives like \u003ccode\u003e/model opus\u003c/code\u003e\u003c/li\u003e\n          \u003cli\u003e\u003cstrong\u003eAuth profile rotation\u003c/strong\u003e — multiple API keys for the same provider, with automatic rotation on rate limits\u003c/li\u003e\n          \u003cli\u003e\u003cstrong\u003eAutomatic failover\u003c/strong\u003e — if a model call fails (rate limit, timeout, context overflow), the Runner retries with fallback models or providers\u003c/li\u003e\n          \u003cli\u003e\u003cstrong\u003eThinking/reasoning levels\u003c/strong\u003e — configurable reasoning depth (off, low, medium, high) that maps to provider-specific extended thinking features\u003c/li\u003e\n          \u003cli\u003e\u003cstrong\u003eContext window management\u003c/strong\u003e — automatic compaction when conversation history approaches the model's context limit\u003c/li\u003e\n        \u003c/ul\u003e\n\n        \u003ch2\u003eStreaming \u0026amp; Block Replies\u003c/h2\u003e\n\n        \u003cp\u003eThe Agent Runner streams responses in real time. As the LLM generates tokens, they are:\u003c/p\u003e\n\n        \u003cul\u003e\n          \u003cli\u003e\u003cstrong\u003eChunked into paragraphs\u003c/strong\u003e — the block chunker accumulates tokens and flushes complete paragraphs to the Channel Adapter, so users see responses appearing in natural-feeling blocks rather than character by character\u003c/li\u003e\n          \u003cli\u003e\u003cstrong\u003eFiltered for thinking tags\u003c/strong\u003e — reasoning/thinking content (from models like Claude with extended thinking) is handled according to the configured reasoning mode: suppressed, streamed separately, or included in the reply\u003c/li\u003e\n          \u003cli\u003e\u003cstrong\u003eChecked for reply directives\u003c/strong\u003e — inline directives like model switches, reply-to tags, and status commands are parsed and handled during streaming\u003c/li\u003e\n          \u003cli\u003e\u003cstrong\u003eTracked for tool calls\u003c/strong\u003e — tool call delimiters are detected in the stream so tool execution can begin as soon as the model finishes specifying the call\u003c/li\u003e\n        \u003c/ul\u003e\n\n        \u003ch2\u003eConversation History Management\u003c/h2\u003e\n\n        \u003cp\u003eEach session maintains a transcript file on disk. The Agent Runner manages this transcript across runs:\u003c/p\u003e\n\n        \u003cul\u003e\n          \u003cli\u003e\u003cstrong\u003eHistory limiting\u003c/strong\u003e — only the most recent N turns are included in the LLM context, configurable per agent and per channel\u003c/li\u003e\n          \u003cli\u003e\u003cstrong\u003eCompaction\u003c/strong\u003e — when the transcript grows too large for the context window, the system summarizes older messages and replaces them with a compact summary, preserving key context while freeing token space\u003c/li\u003e\n          \u003cli\u003e\u003cstrong\u003eTool result truncation\u003c/strong\u003e — oversized tool outputs in the history are automatically truncated to prevent context overflow on subsequent runs\u003c/li\u003e\n          \u003cli\u003e\u003cstrong\u003eTranscript repair\u003c/strong\u003e — malformed transcripts (from crashes or interrupted writes) are automatically detected and repaired on load\u003c/li\u003e\n        \u003c/ul\u003e\n\n        \u003ch2\u003eHow It Works in Code\u003c/h2\u003e\n\n        \u003cp\u003eThe Agent Runner is implemented across several key files in \u003ccode\u003esrc/agents/\u003c/code\u003e:\u003c/p\u003e\n\n        \u003cul\u003e\n          \u003cli\u003e\u003ccode\u003epi-embedded-runner/run.ts\u003c/code\u003e — the main entry point: \u003ccode\u003erunEmbeddedPiAgent()\u003c/code\u003e orchestrates the full run lifecycle including auth resolution, failover, and context assembly\u003c/li\u003e\n          \u003cli\u003e\u003ccode\u003epi-embedded-runner/runs.ts\u003c/code\u003e — manages the registry of active runs, message queueing to active sessions, and abort handling\u003c/li\u003e\n          \u003cli\u003e\u003ccode\u003epi-embedded-subscribe.ts\u003c/code\u003e — the streaming subscription layer that processes LLM output events (message chunks, tool calls, reasoning tokens) and dispatches them to the appropriate handlers\u003c/li\u003e\n          \u003cli\u003e\u003ccode\u003epi-embedded-subscribe.handlers.ts\u003c/code\u003e — event handlers for the six LLM event types: message_start, message_update, message_end, tool_execution_start, tool_execution_update, tool_execution_end\u003c/li\u003e\n          \u003cli\u003e\u003ccode\u003esystem-prompt.ts\u003c/code\u003e — the system prompt builder that assembles all context sections\u003c/li\u003e\n          \u003cli\u003e\u003ccode\u003epi-tools.ts\u003c/code\u003e — tool catalog construction: which tools are available, their schemas, and their policy-based filtering\u003c/li\u003e\n          \u003cli\u003e\u003ccode\u003ecompaction.ts\u003c/code\u003e — conversation compaction logic for managing context window limits\u003c/li\u003e\n        \u003c/ul\u003e\n\n        \u003cdiv class=\"callout tip\"\u003e\n          \u003cdiv class=\"callout-title\"\u003eThe Loop Is the Agent\u003c/div\u003e\n          \u003cp\u003eAn LLM by itself is just a text-completion engine. The Agent Runner is what transforms it into an agent — the ability to observe a situation, decide on an action, execute it, observe the result, and decide again. This loop is the fundamental mechanism that enables complex, multi-step tasks like \"research this topic, write a report, and email it to me.\" Each step requires the model to see what happened in the previous step and decide what to do next.\u003c/p\u003e\n        \u003c/div\u003e\n\n        \u003cdiv class=\"action-section\"\u003e\n          \u003ch2\u003eNext\u003c/h2\u003e\n          \u003cul\u003e\n            \u003cli\u003e\u003ca href=\"/architecture/execution-layer\"\u003eExecution Layer\u003c/a\u003e — how tool calls are actually executed\u003c/li\u003e\n            \u003cli\u003e\u003ca href=\"/architecture/lane-queue\"\u003eLane Queue\u003c/a\u003e — what happens before the Runner gets the message\u003c/li\u003e\n            \u003cli\u003e\u003ca href=\"/architecture\"\u003eArchitecture Overview\u003c/a\u003e — see how all five components connect\u003c/li\u003e\n          \u003c/ul\u003e\n        \u003c/div\u003e"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"VA-lAVEn1LekIejhMlH5y\",\"p\":\"\",\"c\":[\"\",\"architecture\",\"agent-runner\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"(authenticated)\",{\"children\":[\"architecture\",{\"children\":[\"agent-runner\",{\"children\":[\"__PAGE__\",{}]}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/56998b7e09014914.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]]}],{\"children\":[\"(authenticated)\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"body\",null,{\"children\":[[\"$\",\"input\",null,{\"type\":\"checkbox\",\"id\":\"menu-toggle\",\"className\":\"menu-toggle\"}],[\"$\",\"label\",null,{\"htmlFor\":\"menu-toggle\",\"className\":\"hamburger\",\"aria-label\":\"Toggle navigation\",\"children\":\"☰\"}],[\"$\",\"label\",null,{\"htmlFor\":\"menu-toggle\",\"className\":\"overlay\"}],[\"$\",\"div\",null,{\"className\":\"site-wrapper\",\"children\":[[\"$\",\"$L4\",null,{}],[\"$\",\"main\",null,{\"className\":\"main-content\",\"children\":[\"$\",\"div\",null,{\"className\":\"content-wrapper\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":\"$0:f:0:1:1:props:children:1:props:children:props:notFound:0:1:props:style\",\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":\"$0:f:0:1:1:props:children:1:props:children:props:notFound:0:1:props:children:props:children:1:props:style\",\"children\":404}],[\"$\",\"div\",null,{\"style\":\"$0:f:0:1:1:props:children:1:props:children:props:notFound:0:1:props:children:props:children:2:props:style\",\"children\":[\"$\",\"h2\",null,{\"style\":\"$0:f:0:1:1:props:children:1:props:children:props:notFound:0:1:props:children:props:children:2:props:children:props:style\",\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}]]}]]}],{\"children\":[\"architecture\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"agent-runner\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$5\"}}],null,\"$L6\"]}],{},null,false]},null,false]},null,false]},null,false]},null,false],\"$L7\",false]],\"m\":\"$undefined\",\"G\":[\"$8\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"9:I[4431,[],\"OutletBoundary\"]\nb:I[5278,[],\"AsyncMetadataOutlet\"]\nd:I[4431,[],\"ViewportBoundary\"]\nf:I[4431,[],\"MetadataBoundary\"]\n10:\"$Sreact.suspense\"\n6:[\"$\",\"$L9\",null,{\"children\":[\"$La\",[\"$\",\"$Lb\",null,{\"promise\":\"$@c\"}]]}]\n7:[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$Ld\",null,{\"children\":\"$Le\"}],null],[\"$\",\"$Lf\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$10\",null,{\"fallback\":null,\"children\":\"$L11\"}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"e:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\na:null\n"])</script><script>self.__next_f.push([1,"c:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Agent Runner — OpenClaw Architecture\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Your personal AI agent team, working together to keep your life organized.\"}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"11:\"$c:metadata\"\n"])</script></body></html>